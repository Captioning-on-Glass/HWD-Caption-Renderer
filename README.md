# cog-ip-glasses

This is the companion Android app to the Group Captioning Experiment video display found at https://github.com/Captioning-on-Glass/Group-Conversation-Simulation.  It is not meant to be used out of an experimental context.

The Group Captioning Experiment consists of two parts.  One is a C app running a captioned video on a monitor, and the other is an Android app running on some headworn display(eg. Google Glass EE2).  The main idea is that the headworn display transmits the participant's head azimuth position to the C app over Wireless, and the C app uses the information to determine where the participant is facing, and moves the caption on the video into the user's line of sight.

Components of this project:

Flatbuffer
QR Code
Sensor Positions

We begin our journey in MainActivity, which starts a QR code scanner.  We are looking for a QR code generated by the C app which contains the host and port that we want to connect to in order to talk with the C app.  Once we scan the QR code, we jump into CaptioningActivity and pass it the host and port information.  

Inside CaptioningActivity, we connect to the server hosted by the C app and start sending our sensor positions(azimuth) in the form of Datagrams. The orientation messages are serialized using the flatbuffer serialization library.  

Simultaneously, the CaptioningActivity registers a callback, onSensorChanged, so that every time our sensor readings change, we update our rotation matrix and use it to update our orientation, which is what we are sending to the server.  The rotation matrix tells you how to map the coordinate system of the phone to the real world coordinate system.
